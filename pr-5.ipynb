{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bPbH-rlUXtG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((3, 3), dtype=int)\n",
        "        self.current_player = 1\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((3, 3), dtype=int)\n",
        "        self.current_player = 1\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        return tuple(self.board.flatten())\n",
        "\n",
        "    def make_move(self, row, col):\n",
        "        if self.board[row, col] == 0:\n",
        "            self.board[row, col] = self.current_player\n",
        "            winner = self.check_winner()\n",
        "            self.current_player = 3 - self.current_player\n",
        "            return winner\n",
        "        return None\n",
        "\n",
        "    def check_winner(self):\n",
        "        for i in range(3):\n",
        "            if np.all(self.board[i, :] == self.current_player) or np.all(self.board[:, i] == self.current_player):\n",
        "                return self.current_player\n",
        "        if self.board[0, 0] == self.board[1, 1] == self.board[2, 2] == self.current_player:\n",
        "            return self.current_player\n",
        "        if self.board[0, 2] == self.board[1, 1] == self.board[2, 0] == self.current_player:\n",
        "            return self.current_player\n",
        "        if np.all(self.board != 0):\n",
        "            return 0\n",
        "        return None\n",
        "\n",
        "    def get_valid_actions(self):\n",
        "        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]\n",
        "\n",
        "    def render(self):\n",
        "        symbols = {0: '.', 1: 'X', 2: 'O'}\n",
        "        print(\"\\n\".join(\" \".join(symbols[cell] for cell in row) for row in self.board))\n",
        "        print()\n",
        "\n",
        "# Q-Learning\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.2):\n",
        "        self.q_table = {}\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        return self.q_table.get((state, action), 0.0)\n",
        "\n",
        "    def choose_action(self, state, valid_actions):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return valid_actions[np.random.randint(len(valid_actions))]\n",
        "        q_values = [self.get_q_value(state, action) for action in valid_actions]\n",
        "        max_q = max(q_values)\n",
        "        return valid_actions[q_values.index(max_q)]\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state, next_valid_actions):\n",
        "        current_q = self.get_q_value(state, action)\n",
        "        max_next_q = max([self.get_q_value(next_state, a) for a in next_valid_actions], default=0.0)\n",
        "        self.q_table[(state, action)] = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
        "\n",
        "# Обучение\n",
        "def train_agent(episodes):\n",
        "    env = TicTacToe()\n",
        "    agent = QLearningAgent()\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            valid_actions = env.get_valid_actions()\n",
        "            action = agent.choose_action(state, valid_actions)\n",
        "\n",
        "            winner = env.make_move(*action)\n",
        "            env.render()\n",
        "            next_state = env.get_state()\n",
        "            next_valid_actions = env.get_valid_actions()\n",
        "\n",
        "            if winner is None:\n",
        "                reward = 0\n",
        "            elif winner == 0:\n",
        "                reward = 0.5\n",
        "                done = True\n",
        "            elif winner == 1:\n",
        "                reward = 1\n",
        "                done = True\n",
        "            else:\n",
        "                reward = -1\n",
        "                done = True\n",
        "\n",
        "            agent.update_q_value(state, action, reward, next_state, next_valid_actions)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "if name == \"__main__\":\n",
        "    episodes = 1000\n",
        "    rewards = train_agent(episodes)\n",
        "\n",
        "    plt.plot(range(episodes), rewards)\n",
        "    plt.xlabel(\"Эпизоды\")\n",
        "    plt.ylabel(\"Награда\")\n",
        "    plt.title(\"Зависимость награды от числа эпизодов\")\n",
        "    plt.show()"
      ]
    }
  ]
}